{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banxico Parte 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ocupando **únicamente** pandas, numpy; matplotlib; seaborn, con la información obtenida en la parte 1 (pandas DataFrame) modelar con Naïve Bayes si un documento dado pertence a la clase `mantiene`, `sube`, `baja`\n",
    "\n",
    "## Pasos\n",
    "1. Descargar PDFs\n",
    "2. Limpiar los datos\n",
    "3. Eliminar de todos los archivo las palabras que contengan `mantiene`, `sube`, `baja`.\n",
    "4. Parte los documentos entre un set de entrenamiento (0.8) y uno de prueba (0.2)\n",
    "5. Entrena el modelo con los documentos de entrenamiento y valida el resultado con una matriz de confusión usado la base de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os \n",
    "from collections import defaultdict\n",
    "import PyPDF2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from unidecode import unidecode\n",
    "from nltk import corpus\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-13</th>\n",
       "      <td>El objetivo para la Tasa de Inter�s Interbanca...</td>\n",
       "      <td>https://www.banxico.org.mx/publicaciones-y-pre...</td>\n",
       "      <td>disminuye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-19</th>\n",
       "      <td>El objetivo para la Tasa de Inter�s Interbanca...</td>\n",
       "      <td>https://www.banxico.org.mx/publicaciones-y-pre...</td>\n",
       "      <td>disminuye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-14</th>\n",
       "      <td>El objetivo para la Tasa de Inter�s Interbanca...</td>\n",
       "      <td>https://www.banxico.org.mx/publicaciones-y-pre...</td>\n",
       "      <td>disminuye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-26</th>\n",
       "      <td>El objetivo para la Tasa de Inter�s Interbanca...</td>\n",
       "      <td>https://www.banxico.org.mx/publicaciones-y-pre...</td>\n",
       "      <td>disminuye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-15</th>\n",
       "      <td>El objetivo para la Tasa de Inter�s Interbanca...</td>\n",
       "      <td>https://www.banxico.org.mx/publicaciones-y-pre...</td>\n",
       "      <td>disminuye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text  \\\n",
       "date                                                            \n",
       "2020-02-13  El objetivo para la Tasa de Inter�s Interbanca...   \n",
       "2019-12-19  El objetivo para la Tasa de Inter�s Interbanca...   \n",
       "2019-11-14  El objetivo para la Tasa de Inter�s Interbanca...   \n",
       "2019-09-26  El objetivo para la Tasa de Inter�s Interbanca...   \n",
       "2019-08-15  El objetivo para la Tasa de Inter�s Interbanca...   \n",
       "\n",
       "                                                          url   category  \n",
       "date                                                                      \n",
       "2020-02-13  https://www.banxico.org.mx/publicaciones-y-pre...  disminuye  \n",
       "2019-12-19  https://www.banxico.org.mx/publicaciones-y-pre...  disminuye  \n",
       "2019-11-14  https://www.banxico.org.mx/publicaciones-y-pre...  disminuye  \n",
       "2019-09-26  https://www.banxico.org.mx/publicaciones-y-pre...  disminuye  \n",
       "2019-08-15  https://www.banxico.org.mx/publicaciones-y-pre...  disminuye  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"comunicados-banxico-final.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descargamos  los comunicados (ya habíamos probado en la primera parte de la tarea)\n",
    "for i in range(len(df)):\n",
    "    url = df['url'][i]\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    paper = r.content\n",
    "\n",
    "    # recomendable primero crear una carpeta si no existe donde guardaré mis pdfs\n",
    "    if not os.path.exists(\"comunicados_pdfs\"):  \n",
    "        os.mkdir(\"comunicados_pdfs\")\n",
    "    \n",
    "    full_path = os.path.join(\"comunicados_pdfs\", f\"paper{i}.pdf\")\n",
    "    \n",
    "    # escribe cada pdf en mi carpeta\n",
    "    with open(full_path,\"wb\") as f: # wb write binary\n",
    "        f.write(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "#Leemos los papers y los guardamos en un diccionario \n",
    "d = defaultdict(list)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    full_path = os.path.join(\"comunicados_pdfs\", f\"paper{i}.pdf\")\n",
    "    \n",
    "    pdf_file = open(full_path, 'rb')\n",
    "    read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    number_of_pages = read_pdf.getNumPages()\n",
    "    c = Counter(range(number_of_pages))\n",
    "\n",
    "    text_paper_i = ''\n",
    "    for j in c:\n",
    "        page = read_pdf.getPage(j)\n",
    "        page_content = page.extractText()\n",
    "        text_paper_i = text_paper_i + page_content # leemos el pdf completo y lo guardamos en text_paper_i\n",
    "            \n",
    "    d[df['category'][i]].append(text_paper_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizamos como llaves de muestro diccionario las categorias que  se asignaron en la primera parte del proyecto \n",
    "k1, k2, k3 = 'mantiene', 'incrementa', 'disminuye'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una función que nos ayuda a quitar cualquier caracter especial \n",
    "def clean_text(texto):\n",
    "    cln = texto.lower()\n",
    "    cln = re.sub(\"[^\\w ]\", \"\", cln) \n",
    "    return cln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D1 = [clean_text(doc) for doc in d[k1]]\n",
    "D2 = [clean_text(doc) for doc in d[k2]]\n",
    "D3 = [clean_text(doc) for doc in d[k3]]\n",
    "\n",
    "# quiero partir mis datos en un train group y un test group de datos\n",
    "\n",
    "D1, D1_test = train_test_split(D1,test_size = 0.2)\n",
    "D2, D2_test = train_test_split(D2,test_size = 0.2)\n",
    "D3, D3_test = train_test_split(D3,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = len(D1)\n",
    "N2 = len(D2)\n",
    "N3 = len(D3)\n",
    "N = N1 + N2 + N3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continuamos limpiando el texto, quitando las stopwords, y las palabras que no nos intersan  \n",
    "stopwords = corpus.stopwords.words(\"spanish\")\n",
    "key_related_stopwords = ['mantener','disminuir','incrementar', 'mantengan','disminuyan','incrementen']\n",
    "stopwords = stopwords + key_related_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "V1 = [word for txt in D1 for word in txt.split() if word not in stopwords]\n",
    "V2 = [word for txt in D2 for word in txt.split() if word not in stopwords]\n",
    "V3 = [word for txt in D3 for word in txt.split() if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1_c = pd.Series(V1, name='k1').value_counts()\n",
    "V2_c = pd.Series(V2, name = 'k2').value_counts()\n",
    "V3_c = pd.Series(V3, name = 'k3').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = set(V1) | set(V2) | set(V3)\n",
    "v = pd.DataFrame(index=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = (v.join(V1_c, how = \"left\")\n",
    "    .join(V2_c, how = \"left\")\n",
    "    .join(V3_c, how = \"left\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k1</th>\n",
       "      <th>k2</th>\n",
       "      <th>k3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>divergente</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>implementan</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>existían</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>especial</th>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utilizan</th>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>criterios</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>substancial</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vías</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contraído</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   k1        k2        k3\n",
       "divergente   0.000061  0.000158  0.000125\n",
       "implementan  0.000061  0.000079  0.000125\n",
       "existían     0.000061  0.000079  0.000125\n",
       "1198         0.000061  0.000079  0.000125\n",
       "especial     0.000637  0.000789  0.000249\n",
       "utilizan     0.000091  0.000237  0.000125\n",
       "criterios    0.000061  0.000079  0.000125\n",
       "substancial  0.000030  0.000079  0.000249\n",
       "vías         0.000061  0.000079  0.000125\n",
       "contraído    0.000030  0.000079  0.000249"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Funcion de verosimilitud\n",
    "\n",
    "L = L.fillna(0) + 1  \n",
    "L = L / L.sum(axis=0) \n",
    "\n",
    "L.sort_values(\"k1\",ascending = False)\n",
    "L.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion de log-verosimilitud\n",
    "log_L = np.log(L)\n",
    "\n",
    "log_prior_k1 = np.log(N1/N)\n",
    "log_prior_k2 = np.log(N2/N)\n",
    "log_prior_k3 = np.log(N3/N)\n",
    "\n",
    "log_priors = np.array([log_prior_k1,log_prior_k2,log_prior_k3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(D1_test)\n",
    "d_test = D1_test[0]\n",
    "f = set([wrd for wrd in d_test.split() if wrd in V])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k1   -2252.002225\n",
       "k2   -2377.143994\n",
       "k3   -2359.301575\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = log_L.loc[f].sum(axis=0) + log_priors\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "D_test = D1_test + D2_test + D3_test\n",
    "print(len(D_test))\n",
    "y_test = [\"k1\"]*len(D1_test) + [\"k2\"]*len(D2_test) + [\"k3\"]*len(D3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k1', 'k2', 'k2', 'k2', 'k2', 'k3', 'k3', 'k3', 'k3']\n",
      "['k1', 'k1', 'k1', 'k1', 'k3', 'k2', 'k1', 'k1', 'k1', 'k3', 'k3', 'k1', 'k1', 'k1', 'k1', 'k2', 'k2', 'k2', 'k1', 'k2', 'k2', 'k2', 'k3']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "yhat_test = [] \n",
    "for d_test in D_test:\n",
    "    f = set([palabra for palabra in d_test.split() if palabra in V])\n",
    "    res = log_L.loc[f].sum(axis=0) + log_priors\n",
    "    yhat_test.append(res.idxmax())\n",
    "\n",
    "print(y_test)\n",
    "print(yhat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  1,  3],\n",
       "       [ 1,  3,  0],\n",
       "       [ 0,  3,  1]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,yhat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
